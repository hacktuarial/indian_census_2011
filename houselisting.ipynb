{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape 2011 Indian Census Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Houselisting\n",
    "[link](http://www.censusindia.gov.in/2011census/HLO/HL_PCA/Houselisting-housing-Gujarat.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    page = requests.get(url)\n",
    "    return BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_root = \"/Users/timothysweetser/Downloads/houselisting\"\n",
    "os.mkdir(output_root)\n",
    "root_url = \"http://www.censusindia.gov.in/2011census/HLO/HL_PCA\"\n",
    "top_url = \"{}/{}\".format(root_url, \"Houselisting-housing-HLPCA.html\")\n",
    "n_provinces = 35 # manually counted these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_soup = make_soup(top_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Andaman and Nicobar Islands UT', u'Haryana', u'Nagaland', u'Andhra Pradesh ', u'Himachal Pradesh', u'Odisha', u'Arunachal Pradesh', u'Jammu & Kashmir', u'Puducherry UT', u'Assam', u'Jharkhand', u' Punjab', u' Bihar', u'Karnataka', u'Rajasthan ', u' Chandigarh UT', u'Kerala', u'Sikkim', u' Chhattisgarh', u'Lakshadweep UT', u'Tamil Nadu', u'Dadra & Nagar Haveli UT', u'Madhya Pradesh ', u'Tripura', u'Daman & Diu UT ', u'Maharashtra ', u'Uttar Pradesh', u'NCT of Delhi', u'Manipur', u'Uttarakhand', u' Goa', u'Meghalaya', u' West Bengal', u'Gujarat', u'Mizoram']\n"
     ]
    }
   ],
   "source": [
    "provinces = map(lambda x: x.get_text(), top_soup.findAll(\"td\"))[12:-5]\n",
    "assert len(provinces) == n_provinces\n",
    "print provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'http://www.censusindia.gov.in/2011census/HLO/HL_PCA/Houselisting-housing-Andaman.html', u'http://www.censusindia.gov.in/2011census/HLO/HL_PCA/Houselisting-housing-HARYANA.html', u'http://www.censusindia.gov.in/2011census/HLO/HL_PCA/Houselisting-housing-NAGALAND.html', u'http://www.censusindia.gov.in/2011census/HLO/HL_PCA/Houselisting-housing-AP.html']\n"
     ]
    }
   ],
   "source": [
    "# extract links to each province's site\n",
    "province_urls = [x['href'] for x in top_soup.findAll(\"a\", href=True)]\n",
    "province_urls = [\"/\".join(top_url.split(\"/\")[:-1]) + \"/%s\" % x for x in province_urls]\n",
    "print province_urls[0:4]\n",
    "assert len(province_urls) == n_provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count number of subprovinces to make sure we download the right number of files\n",
    "n_subprovinces = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andaman and Nicobar Islands UT\n",
      "Haryana\n",
      "Nagaland\n",
      "Andhra Pradesh \n",
      "Himachal Pradesh\n",
      "Odisha\n",
      "Arunachal Pradesh\n",
      "Jammu & Kashmir\n",
      "Puducherry UT\n",
      "Assam\n",
      "Jharkhand\n",
      " Punjab\n",
      " Bihar\n",
      "Karnataka\n",
      "Rajasthan \n",
      " Chandigarh UT\n",
      "Kerala\n",
      "Sikkim\n",
      " Chhattisgarh\n",
      "Lakshadweep UT\n",
      "Tamil Nadu\n",
      "Dadra & Nagar Haveli UT\n",
      "Madhya Pradesh \n",
      "Tripura\n",
      "Daman & Diu UT \n",
      "Maharashtra \n",
      "Uttar Pradesh\n",
      "NCT of Delhi\n",
      "Manipur\n",
      "Uttarakhand\n",
      " Goa\n",
      "Meghalaya\n",
      " West Bengal\n",
      "Gujarat\n",
      "Mizoram\n"
     ]
    }
   ],
   "source": [
    "# loop over provinces\n",
    "for i in xrange(n_provinces):\n",
    "    province = provinces[i]\n",
    "    print province\n",
    "    # load this province's site, e.g. http://censusindia.gov.in/pca/pcadata/Houselisting-housing-Andaman.html\n",
    "    soup = make_soup(province_urls[i])\n",
    "    # get links to each subprovince and count them\n",
    "    subprovince_urls = soup.findAll(\"a\", href=True)\n",
    "    n_subprovinces[province] = len(subprovince_urls)\n",
    "    \n",
    "    # get the URL of the target excel sheet\n",
    "    subprovince_urls = [\"{}/{}\".format(root_url, x[\"href\"]) for x in subprovince_urls]\n",
    "    excel_sheets = [requests.get(url) for url in subprovince_urls]\n",
    "    \n",
    "    # download each excel sheet and write to file\n",
    "    for j in xrange(len(excel_sheets)):\n",
    "        address = \"{root}/{province}_{index}.xlsx\".format(**{\n",
    "                \"root\": output_root,\n",
    "                \"province\": province.replace(\" \", \"_\"),\n",
    "                \"index\": j\n",
    "                })\n",
    "        output = open(address, 'wb')\n",
    "        output.write(excel_sheets[j].content)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we downloaded the right number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_files_on_disk = len([x for x in os.listdir(output_root) if \".xlsx\" in x ])\n",
    "n_files_on_web = reduce(lambda x, y: x+y, n_subprovinces.itervalues())\n",
    "assert n_files_on_disk == n_files_on_web"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
